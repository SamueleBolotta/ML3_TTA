{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDYnDRjPfNF_"
   },
   "source": [
    "# DL Project 2023/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ll5uAwzhfYHn"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Description of the method choosen and the work done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision matplotlib tqdm Pillow numpy --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 19732,
     "status": "ok",
     "timestamp": 1715970600487,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "Lw7k2mxRfE1D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1486330/1096835347.py:39: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.23.0)\n",
      "  import scipy.ndimage as ndimage\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch and related modules\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Import torchvision and related modules\n",
    "import torchvision\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import multiprocessing\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import scipy.ndimage as ndimage\n",
    "import copy\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37897,
     "status": "ok",
     "timestamp": 1715970642796,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "qQnIh3lo3VAS",
    "outputId": "94b3fb3e-e93d-4bf2-eceb-79b231353554"
   },
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "    tar_file = \"/content/drive/MyDrive/DL_project/imagenet-a.tar\"\n",
    "else:\n",
    "    # Set the path for Jupyter \n",
    "    tar_file = \"./imagenet-a.tar\"\n",
    "\n",
    "data_folder = \"imagenet-a\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqX-jwMHgZ3G"
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21703,
     "status": "ok",
     "timestamp": 1715970667457,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "4h7Y7SSrgc7y",
    "outputId": "bd874e65-9065-46ec-947e-8a4ae09e067e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the data is extracted.\n"
     ]
    }
   ],
   "source": [
    "# function to untar the dataset and store it in a new folder\n",
    "def extract_dataset(compress_file, destination_folder):\n",
    "  # function to change dir names to their words description\n",
    "  def change_folders_names(readme_file, dataset_root):\n",
    "    with open(readme_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # Match lines containing WordNet IDs and descriptions\n",
    "            match = re.match(r'n\\d+ (.+)', line)\n",
    "            if match:\n",
    "                # Split the line into WordNet ID and description\n",
    "                parts = match.group(0).split()\n",
    "                wordnet_id = parts[0]\n",
    "                description = ' '.join(parts[1:])\n",
    "                os.rename(os.path.join(dataset_root, wordnet_id),\n",
    "                            os.path.join(dataset_root, description))\n",
    "\n",
    "  if not os.path.exists(compress_file):\n",
    "    print(\"Compress file doesn't exist.\")\n",
    "    return\n",
    "\n",
    "  if os.path.exists(destination_folder):\n",
    "    # remove the folder if already exists one\n",
    "    shutil.rmtree(destination_folder)\n",
    "\n",
    "  # extract content from the .tar file\n",
    "  with tarfile.open(compress_file, 'r') as tar_ref:\n",
    "    tar_ref.extractall(\"./\")\n",
    "  print(\"All the data is extracted.\")\n",
    "\n",
    "  change_folders_names(destination_folder+\"/README.txt\", destination_folder)\n",
    "\n",
    "extract_dataset(tar_file, data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 592,
     "status": "ok",
     "timestamp": 1715970670636,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "rzBAWU06L_Mm",
    "outputId": "2a005eca-33f2-45bb-ea03-a7427ee7fa99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_list = os.listdir(data_folder)\n",
    "len(ids_list) # 200 folders + 1 readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tweaked function to test the models on a subset of the data\n",
    "\n",
    "def get_data_subset(batch_size, dataset_path, transform):\n",
    "    # Load the entire dataset\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "    \n",
    "    # Get the class labels\n",
    "    class_labels = data.classes\n",
    "    print(f\"The dataset contains {len(data)} images.\")\n",
    "    print(f\"The dataset contains {len(class_labels)} labels.\")\n",
    "    \n",
    "    # Create a subset with only the first 1000 images\n",
    "    subset_indices = list(range(10))  # Indices of the first 1000 images\n",
    "    data_subset = torch.utils.data.Subset(data, subset_indices)\n",
    "    \n",
    "    # Create a DataLoader for the subset\n",
    "    test_loader = torch.utils.data.DataLoader(data_subset, batch_size=batch_size, shuffle=False, num_workers=multiprocessing.cpu_count())\n",
    "    \n",
    "    return test_loader, class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1715970673171,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "eul04CxP3VaH"
   },
   "outputs": [],
   "source": [
    "# function that returns a DataLoader for the dataset\n",
    "def get_data(batch_size, dataset_path, transform):\n",
    "\n",
    "  data = torchvision.datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "  class_labels = data.classes\n",
    "  print(f\"The dataset contains {len(data)} images.\")\n",
    "  print(f\"The dataset contains {len(class_labels)} labels.\")\n",
    "\n",
    "  test_loader = torch.utils.data.DataLoader(data, batch_size, shuffle=False, num_workers=multiprocessing.cpu_count())\n",
    "\n",
    "  return test_loader, class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1715970676875,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "HHAElKCP7mMv"
   },
   "outputs": [],
   "source": [
    "# function to display images from the DataLoader\n",
    "def show_images(dataloader, num_images=5):\n",
    "  # get a batch of data\n",
    "  data_iter = iter(dataloader)\n",
    "  images, labels = next(data_iter)\n",
    "\n",
    "  # convert images to numpy array\n",
    "  images = images.numpy()\n",
    "\n",
    "  # display images\n",
    "  fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
    "  for i in range(num_images):\n",
    "      image = np.transpose(images[i], (1, 2, 0))  # move channels in last position\n",
    "      image = np.clip(image, 0, 1)\n",
    "      axes[i].imshow(image)\n",
    "      axes[i].axis('off')\n",
    "      axes[i].set_title(dataloader.dataset.classes[labels[i]])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code defines various classes and functions to implement Recurrent Independent Mechanisms (RIMs), a specialized type of neural network layer that can handle multiple independent recurrent units simultaneously.\n",
    "\n",
    "1. **Blocked gradient function:** This custom function blocks gradients for specific elements during backpropagation, allowing fine-grained control over which parts of the model update their weights.\n",
    "\n",
    "2. **Group linear and recurrent layers:** \n",
    "   - **GroupLinearLayer:** A linear layer that processes multiple blocks of data in parallel.\n",
    "   - **GroupLSTMCell and GroupGRUCell:** Custom LSTM and GRU cells that process multiple sequences simultaneously, improving computational efficiency.\n",
    "\n",
    "3. **RIMCell class:** This class integrates the independent recurrent units, input attention, and communication mechanisms. It dynamically selects which units are active and allows them to communicate selectively.\n",
    "\n",
    "4. **RIM class:** This class stacks multiple RIM cells into layers and handles the forward pass through these layers, enabling the model to process sequences of data while maintaining the independence of recurrent units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class blocked_grad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, mask):\n",
    "        ctx.save_for_backward(x, mask)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, mask = ctx.saved_tensors\n",
    "        return grad_output * mask, mask * 0.0\n",
    "\n",
    "class GroupLinearLayer(nn.Module):\n",
    "    def __init__(self, din, dout, num_blocks):\n",
    "        super(GroupLinearLayer, self).__init__()\n",
    "        self.w = nn.Parameter(0.01 * torch.randn(num_blocks, din, dout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = torch.bmm(x, self.w)\n",
    "        return x.permute(1, 0, 2)\n",
    "\n",
    "class GroupLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    GroupLSTMCell can compute the operation of N LSTM Cells at once.\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_size, hidden_size, num_lstms):\n",
    "        super().__init__()\n",
    "        self.inp_size = inp_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = GroupLinearLayer(inp_size, 4 * hidden_size, num_lstms)\n",
    "        self.h2h = GroupLinearLayer(hidden_size, 4 * hidden_size, num_lstms)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, hid_state):\n",
    "        \"\"\"\n",
    "        input: x (batch_size, num_lstms, input_size)\n",
    "               hid_state (tuple of length 2 with each element of size (batch_size, num_lstms, hidden_state))\n",
    "        output: h (batch_size, num_lstms, hidden_state)\n",
    "                c ((batch_size, num_lstms, hidden_state))\n",
    "        \"\"\"\n",
    "        h, c = hid_state\n",
    "        preact = self.i2h(x) + self.h2h(h)\n",
    "        gates = preact[:, :, :3 * self.hidden_size].sigmoid()\n",
    "        g_t = preact[:, :, 3 * self.hidden_size:].tanh()\n",
    "        i_t = gates[:, :, :self.hidden_size]\n",
    "        f_t = gates[:, :, self.hidden_size:2 * self.hidden_size]\n",
    "        o_t = gates[:, :, -self.hidden_size:]\n",
    "        c_t = torch.mul(c, f_t) + torch.mul(i_t, g_t)\n",
    "        h_t = torch.mul(o_t, c_t.tanh())\n",
    "        return h_t, c_t\n",
    "\n",
    "class GroupGRUCell(nn.Module):\n",
    "    \"\"\"\n",
    "    GroupGRUCell can compute the operation of N GRU Cells at once.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_grus):\n",
    "        super(GroupGRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.x2h = GroupLinearLayer(input_size, 3 * hidden_size, num_grus)\n",
    "        self.h2h = GroupLinearLayer(hidden_size, 3 * hidden_size, num_grus)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data = torch.ones(w.data.size())\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        input: x (batch_size, num_grus, input_size)\n",
    "               hidden (batch_size, num_grus, hidden_size)\n",
    "        output: hidden (batch_size, num_grus, hidden_size)\n",
    "        \"\"\"\n",
    "        gate_x = self.x2h(x)\n",
    "        gate_h = self.h2h(hidden)\n",
    "        i_r, i_i, i_n = gate_x.chunk(3, 2)\n",
    "        h_r, h_i, h_n = gate_h.chunk(3, 2)\n",
    "        resetgate = torch.sigmoid(i_r + h_r)\n",
    "        inputgate = torch.sigmoid(i_i + h_i)\n",
    "        newgate = torch.tanh(i_n + (resetgate * h_n))\n",
    "        hy = newgate + inputgate * (hidden - newgate)\n",
    "        return hy\n",
    "\n",
    "class RIMCell(nn.Module):\n",
    "    def __init__(self, \n",
    "        device, input_size, hidden_size, num_units, k, rnn_cell, input_key_size = 64, input_value_size = 400, input_query_size = 64,\n",
    "        num_input_heads = 1, input_dropout = 0.1, comm_key_size = 32, comm_value_size = 100, comm_query_size = 32, num_comm_heads = 4, comm_dropout = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if comm_value_size != hidden_size:\n",
    "            comm_value_size = hidden_size\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_units = num_units\n",
    "        self.rnn_cell = rnn_cell\n",
    "        self.key_size = input_key_size\n",
    "        self.k = k\n",
    "        self.num_input_heads = num_input_heads\n",
    "        self.num_comm_heads = num_comm_heads\n",
    "        self.input_key_size = input_key_size\n",
    "        self.input_query_size = input_query_size\n",
    "        self.input_value_size = input_value_size\n",
    "        self.comm_key_size = comm_key_size\n",
    "        self.comm_query_size = comm_query_size\n",
    "        self.comm_value_size = comm_value_size\n",
    "        self.key = nn.Linear(input_size, num_input_heads * input_query_size).to(self.device)\n",
    "        self.value = nn.Linear(input_size, num_input_heads * input_value_size).to(self.device)\n",
    "        if self.rnn_cell == 'GRU':\n",
    "            self.rnn = GroupGRUCell(input_value_size, hidden_size, num_units)\n",
    "            self.query = GroupLinearLayer(hidden_size, input_key_size * num_input_heads, self.num_units)\n",
    "        else:\n",
    "            self.rnn = GroupLSTMCell(input_value_size, hidden_size, num_units)\n",
    "            self.query = GroupLinearLayer(hidden_size, input_key_size * num_input_heads, self.num_units)\n",
    "        self.query_ = GroupLinearLayer(hidden_size, comm_query_size * num_comm_heads, self.num_units)\n",
    "        self.key_ = GroupLinearLayer(hidden_size, comm_key_size * num_comm_heads, self.num_units)\n",
    "        self.value_ = GroupLinearLayer(hidden_size, comm_value_size * num_comm_heads, self.num_units)\n",
    "        self.comm_attention_output = GroupLinearLayer(num_comm_heads * comm_value_size, comm_value_size, self.num_units)\n",
    "        self.comm_dropout = nn.Dropout(p=input_dropout)\n",
    "        self.input_dropout = nn.Dropout(p=comm_dropout)\n",
    "\n",
    "    def transpose_for_scores(self, x, num_attention_heads, attention_head_size):\n",
    "        new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def input_attention_mask(self, x, h):\n",
    "        \"\"\"\n",
    "        Input : x (batch_size, 2, input_size) [The null input is appended along the first dimension]\n",
    "                h (batch_size, num_units, hidden_size)\n",
    "        Output: inputs (list of size num_units with each element of shape (batch_size, input_value_size))\n",
    "                mask_ binary array of shape (batch_size, num_units) where 1 indicates active and 0 indicates inactive\n",
    "        \"\"\"\n",
    "        key_layer = self.key(x)\n",
    "        value_layer = self.value(x)\n",
    "        query_layer = self.query(h)\n",
    "        key_layer = self.transpose_for_scores(key_layer, self.num_input_heads, self.input_key_size)\n",
    "        value_layer = torch.mean(self.transpose_for_scores(value_layer, self.num_input_heads, self.input_value_size), dim=1)\n",
    "        query_layer = self.transpose_for_scores(query_layer, self.num_input_heads, self.input_query_size)\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) / math.sqrt(self.input_key_size)\n",
    "        attention_scores = torch.mean(attention_scores, dim=1)\n",
    "        mask_ = torch.zeros(x.size(0), self.num_units).to(self.device)\n",
    "        not_null_scores = attention_scores[:, :, 0]\n",
    "        topk1 = torch.topk(not_null_scores, self.k, dim=1)\n",
    "        row_index = np.arange(x.size(0))\n",
    "        row_index = np.repeat(row_index, self.k)\n",
    "        mask_[row_index, topk1.indices.view(-1)] = 1\n",
    "        attention_probs = self.input_dropout(nn.Softmax(dim=-1)(attention_scores))\n",
    "        inputs = torch.matmul(attention_probs, value_layer) * mask_.unsqueeze(2)\n",
    "        return inputs, mask_\n",
    "\n",
    "    def communication_attention(self, h, mask):\n",
    "        \"\"\"\n",
    "        Input : h (batch_size, num_units, hidden_size)\n",
    "                mask obtained from the input_attention_mask() function\n",
    "        Output: context_layer (batch_size, num_units, hidden_size). New hidden states after communication\n",
    "        \"\"\"\n",
    "        query_layer = []\n",
    "        key_layer = []\n",
    "        value_layer = []\n",
    "        query_layer = self.query_(h)\n",
    "        key_layer = self.key_(h)\n",
    "        value_layer = self.value_(h)\n",
    "        query_layer = self.transpose_for_scores(query_layer, self.num_comm_heads, self.comm_query_size)\n",
    "        key_layer = self.transpose_for_scores(key_layer, self.num_comm_heads, self.comm_key_size)\n",
    "        value_layer = self.transpose_for_scores(value_layer, self.num_comm_heads, self.comm_value_size)\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.comm_key_size)\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        mask = [mask for _ in range(attention_probs.size(1))]\n",
    "        mask = torch.stack(mask, dim=1)\n",
    "        attention_probs = attention_probs * mask.unsqueeze(3)\n",
    "        attention_probs = self.comm_dropout(attention_probs)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.num_comm_heads * self.comm_value_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        context_layer = self.comm_attention_output(context_layer)\n",
    "        context_layer = context_layer + h\n",
    "        return context_layer\n",
    "\n",
    "    def forward(self, x, hs, cs=None):\n",
    "        \"\"\"\n",
    "        Input : x (batch_size, 1 , input_size)\n",
    "                hs (batch_size, num_units, hidden_size)\n",
    "                cs (batch_size, num_units, hidden_size)\n",
    "        Output: new hs, cs for LSTM\n",
    "                new hs for GRU\n",
    "        \"\"\"\n",
    "        size = x.size()\n",
    "        null_input = torch.zeros(size[0], 1, size[2]).float().to(self.device)\n",
    "        x = torch.cat((x, null_input), dim=1)\n",
    "        inputs, mask = self.input_attention_mask(x, hs)\n",
    "        h_old = hs * 1.0\n",
    "        if cs is not None:\n",
    "            c_old = cs * 1.0\n",
    "        if cs is not None:\n",
    "            hs, cs = self.rnn(inputs, (hs, cs))\n",
    "        else:\n",
    "            hs = self.rnn(inputs, hs)\n",
    "        mask = mask.unsqueeze(2)\n",
    "        h_new = blocked_grad.apply(hs, mask)\n",
    "        h_new = self.communication_attention(h_new, mask.squeeze(2))\n",
    "        hs = mask * h_new + (1 - mask) * h_old\n",
    "        if cs is not None:\n",
    "            cs = mask * cs + (1 - mask) * c_old\n",
    "            return hs, cs\n",
    "        return hs, None\n",
    "\n",
    "class RIM(nn.Module):\n",
    "    def __init__(self, device, input_size, hidden_size, num_units, k, rnn_cell, n_layers, bidirectional, **kwargs):\n",
    "        super().__init__()\n",
    "        if device == 'cuda':\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        self.n_layers = n_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.rnn_cell = rnn_cell\n",
    "        self.num_units = num_units\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.num_directions == 2:\n",
    "            self.rimcell = nn.ModuleList([RIMCell(self.device, input_size, hidden_size, num_units, k, rnn_cell, **kwargs).to(self.device) if i < 2 else\n",
    "                                          RIMCell(self.device, 2 * hidden_size * self.num_units, hidden_size, num_units, k, rnn_cell, **kwargs).to(self.device) for i in range(self.n_layers * self.num_directions)])\n",
    "        else:\n",
    "            self.rimcell = nn.ModuleList([RIMCell(self.device, input_size, hidden_size, num_units, k, rnn_cell, **kwargs).to(self.device) if i == 0 else\n",
    "                                          RIMCell(self.device, hidden_size * self.num_units, hidden_size, num_units, k, rnn_cell, **kwargs).to(self.device) for i in range(self.n_layers)])\n",
    "\n",
    "    def layer(self, rim_layer, x, h, c=None, direction=0):\n",
    "        batch_size = x.size(1)\n",
    "\n",
    "        # Debugging: Print input shapes\n",
    "        #print(f\"layer - Initial x shape: {x.shape}\")\n",
    "        #print(f\"layer - Initial h shape: {h.shape}\")\n",
    "        #if c is not None:\n",
    "            #print(f\"layer - Initial c shape: {c.shape}\")\n",
    "\n",
    "        xs = list(torch.split(x, 1, dim=0))\n",
    "        if direction == 1:\n",
    "            xs.reverse()\n",
    "\n",
    "        hs = h.squeeze(0).view(batch_size, self.num_units, -1)\n",
    "\n",
    "        # Debugging: Print reshaped hs shape\n",
    "        #print(f\"layer - Reshaped hs shape: {hs.shape}\")\n",
    "\n",
    "        cs = None\n",
    "        if c is not None:\n",
    "            cs = c.squeeze(0).view(batch_size, self.num_units, -1)\n",
    "            # Debugging: Print reshaped cs shape\n",
    "            #print(f\"layer - Reshaped cs shape: {cs.shape}\")\n",
    "\n",
    "        outputs = []\n",
    "        for x in xs:\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "            # Debugging: Print shape before rim_layer call\n",
    "            #print(f\"layer - x shape before rim_layer: {x.shape}\")\n",
    "            #print(f\"layer - hs shape before rim_layer: {hs.shape}\")\n",
    "            #if cs is not None:\n",
    "                #print(f\"layer - cs shape before rim_layer: {cs.shape}\")\n",
    "\n",
    "            hs, cs = rim_layer(x.unsqueeze(1), hs, cs)\n",
    "\n",
    "            # Debugging: Print shape after rim_layer call\n",
    "            #print(f\"layer - hs shape after rim_layer: {hs.shape}\")\n",
    "            #if cs is not None:\n",
    "                #print(f\"layer - cs shape after rim_layer: {cs.shape}\")\n",
    "\n",
    "            outputs.append(hs.view(1, batch_size, -1))\n",
    "\n",
    "        if direction == 1:\n",
    "            outputs.reverse()\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "\n",
    "        # Debugging: Print final outputs shape\n",
    "        #print(f\"layer - Final outputs shape: {outputs.shape}\")\n",
    "\n",
    "        if c is not None:\n",
    "            return outputs, hs.view(batch_size, -1), cs.view(batch_size, -1)\n",
    "        else:\n",
    "            return outputs, hs.view(batch_size, -1)\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        \"\"\"\n",
    "        Input: x (seq_len, batch_size, feature_size)\n",
    "               h (num_layers * num_directions, batch_size, hidden_size * num_units)\n",
    "               c (num_layers * num_directions, batch_size, hidden_size * num_units)\n",
    "        Output: outputs (batch_size, seqlen, hidden_size * num_units * num-directions)\n",
    "                h(and c) (num_layer * num_directions, batch_size, hidden_size* num_units)\n",
    "        \"\"\"\n",
    "\n",
    "        # Flatten the input image to match RIM input shape\n",
    "        # x = x.view(x.size(0), x.size(1), -1)  # (1, 3, 224*224) -> (1, 1, 3*224*224)\n",
    "\n",
    "        hs = torch.split(h, 1, 0) if h is not None else torch.split(torch.randn(self.n_layers * self.num_directions, x.size(1), self.hidden_size * self.num_units).to(self.device), 1, 0)\n",
    "        hs = list(hs)\n",
    "        cs = None\n",
    "        if self.rnn_cell == 'LSTM':\n",
    "            cs = torch.split(c, 1, 0) if c is not None else torch.split(torch.randn(self.n_layers * self.num_directions, x.size(1), self.hidden_size * self.num_units).to(self.device), 1, 0)\n",
    "            cs = list(cs)\n",
    "        for n in range(self.n_layers):\n",
    "            idx = n * self.num_directions\n",
    "            if cs is not None:\n",
    "                x_fw, hs[idx], cs[idx] = self.layer(self.rimcell[idx], x, hs[idx], cs[idx])\n",
    "            else:\n",
    "                x_fw, hs[idx] = self.layer(self.rimcell[idx], x, hs[idx], c=None)\n",
    "            if self.num_directions == 2:\n",
    "                idx = n * self.num_directions + 1\n",
    "                if cs is not None:\n",
    "                    x_bw, hs[idx], cs[idx] = self.layer(self.rimcell[idx], x, hs[idx], cs[idx], direction=1)\n",
    "                else:\n",
    "                    x_bw, hs[idx] = self.layer(self.rimcell[idx], x, hs[idx], c=None, direction=1)\n",
    "                x = torch.cat((x_fw, x_bw), dim=2)\n",
    "            else:\n",
    "                x = x_fw\n",
    "        hs = torch.stack(hs, dim=0)\n",
    "        if cs is not None:\n",
    "            cs = torch.stack(cs, dim=0)\n",
    "            return x, hs, cs\n",
    "        return x, hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxlU46PFgdSd"
   },
   "source": [
    "## MEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code defines a series of image augmentation functions and a list of these augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1715970681225,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "tNxMhrlyj5IW"
   },
   "outputs": [],
   "source": [
    "def convert_to_pil(img):\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = TF.to_pil_image(img)\n",
    "    return img\n",
    "\n",
    "def vertical_flip(img):\n",
    "    img = convert_to_pil(img)\n",
    "    res = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def horizontal_flip(img):\n",
    "    img = convert_to_pil(img)\n",
    "    res = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def brightness(img, factor_range=(0.5, 1.5)):\n",
    "    img = convert_to_pil(img)\n",
    "    factor = np.random.uniform(factor_range[0], factor_range[1])\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    res = enhancer.enhance(factor)\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def color(img, factor_range=(0.5, 1.5)):\n",
    "    img = convert_to_pil(img)\n",
    "    factor = np.random.uniform(factor_range[0], factor_range[1])\n",
    "    enhancer = ImageEnhance.Color(img)\n",
    "    res = enhancer.enhance(factor)\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def sharpness(img, factor_range=(0.5, 1.5)):\n",
    "    img = convert_to_pil(img)\n",
    "    factor = np.random.uniform(factor_range[0], factor_range[1])\n",
    "    enhancer = ImageEnhance.Sharpness(img)\n",
    "    res = enhancer.enhance(factor)\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def rotation(img, angle_range=(-45, 45)):\n",
    "    img = convert_to_pil(img)\n",
    "    angle = np.random.uniform(angle_range[0], angle_range[1])\n",
    "    res = img.rotate(angle)\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def gaussian_blur(img, radius_range=(0.1, 2.0)):\n",
    "    img = convert_to_pil(img)\n",
    "    radius = np.random.uniform(radius_range[0], radius_range[1])\n",
    "    res = img.filter(ImageFilter.GaussianBlur(radius))\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def random_crop(img, size=(64, 64)):  # Default size provided\n",
    "    img = convert_to_pil(img)\n",
    "    width, height = img.size\n",
    "    crop_width, crop_height = size\n",
    "    if width < crop_width or height < crop_height:\n",
    "        raise ValueError(\"Crop size must be less than the image size\")\n",
    "    left = np.random.randint(0, width - crop_width + 1)\n",
    "    top = np.random.randint(0, height - crop_height + 1)\n",
    "    right = left + crop_width\n",
    "    bottom = top + crop_height\n",
    "    res = img.crop((left, top, right, bottom))\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def add_noise(img, mean=0, std=0.1):\n",
    "    if isinstance(img, Image.Image):\n",
    "        img = TF.to_tensor(img)\n",
    "    noise = torch.randn(img.size(), device=img.device) * std + mean\n",
    "    res = img + noise\n",
    "    res = torch.clamp(res, 0, 1)\n",
    "    return res\n",
    "\n",
    "def affine_transform(img, max_translation=(10, 10), max_rotation=30, max_shear=10):\n",
    "    img = convert_to_pil(img)\n",
    "    width, height = img.size\n",
    "    translation = (np.random.uniform(-max_translation[0], max_translation[0]),\n",
    "                   np.random.uniform(-max_translation[1], max_translation[1]))\n",
    "    rotation = np.random.uniform(-max_rotation, max_rotation)\n",
    "    shear = np.random.uniform(-max_shear, max_shear)\n",
    "    res = img.transform((width, height), Image.AFFINE, \n",
    "                        (1, shear, translation[0], shear, 1, translation[1]),\n",
    "                        resample=Image.BILINEAR)\n",
    "    res = res.rotate(rotation)\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def elastic_deformation(img, alpha=34, sigma=4):\n",
    "    img = convert_to_pil(img)\n",
    "    img = np.array(img)\n",
    "    random_state = np.random.RandomState(None)\n",
    "    shape = img.shape\n",
    "    dx = ndimage.gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = ndimage.gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dz = np.zeros_like(dx)\n",
    "    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    "    distored_image = ndimage.map_coordinates(img, indices, order=1, mode='reflect')\n",
    "    return TF.to_tensor(distored_image.reshape(img.shape))\n",
    "\n",
    "def contrast(img, factor_range=(0.5, 1.5)):\n",
    "    img = convert_to_pil(img)\n",
    "    factor = np.random.uniform(factor_range[0], factor_range[1])\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    res = enhancer.enhance(factor)\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def saturation(img, factor_range=(0.5, 1.5)):\n",
    "    img = convert_to_pil(img)\n",
    "    factor = np.random.uniform(factor_range[0], factor_range[1])\n",
    "    enhancer = ImageEnhance.Color(img)\n",
    "    res = enhancer.enhance(factor)\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def hue(img, factor_range=(-0.1, 0.1)):\n",
    "    img = convert_to_pil(img)\n",
    "    factor = np.random.uniform(factor_range[0], factor_range[1])\n",
    "    res = TF.adjust_hue(img, factor)\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def perspective_transform(img, distortion_scale=0.5):\n",
    "    img = convert_to_pil(img)\n",
    "    transform = torchvision.transforms.RandomPerspective(distortion_scale=distortion_scale, p=1.0)\n",
    "    res = transform(img)\n",
    "    return TF.to_tensor(res)\n",
    "\n",
    "def channel_shuffle(img):\n",
    "    if isinstance(img, Image.Image):\n",
    "        img = TF.to_tensor(img)\n",
    "    img = img[torch.randperm(3), :, :]\n",
    "    return img\n",
    "\n",
    "def random_erasing(img, sl=0.02, sh=0.4, r1=0.3, mean=[0.4914, 0.4822, 0.4465]):\n",
    "    if isinstance(img, Image.Image):\n",
    "        img = TF.to_tensor(img)\n",
    "    c, h, w = img.size()\n",
    "    area = h * w\n",
    "\n",
    "    target_area = np.random.uniform(sl, sh) * area\n",
    "    aspect_ratio = np.random.uniform(r1, 1/r1)\n",
    "\n",
    "    h_erase = int(round(np.sqrt(target_area * aspect_ratio)))\n",
    "    w_erase = int(round(np.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "    if h_erase > h:\n",
    "        h_erase = h\n",
    "\n",
    "    if w_erase > w:\n",
    "        w_erase = w\n",
    "\n",
    "    if h_erase > 0 and w_erase > 0:\n",
    "        x1 = np.random.randint(0, h - h_erase + 1)\n",
    "        y1 = np.random.randint(0, w - w_erase + 1)\n",
    "        img[:, x1:x1 + h_erase, y1:y1 + w_erase] = torch.tensor(mean).view(-1, 1, 1)\n",
    "\n",
    "    return img\n",
    "\n",
    "def histogram_equalization(img):\n",
    "    img = convert_to_pil(img)\n",
    "    img = np.array(img)\n",
    "    if len(img.shape) == 3:\n",
    "        for i in range(3):\n",
    "            img[:,:,i] = cv2.equalizeHist(img[:,:,i])\n",
    "    else:\n",
    "        img = cv2.equalizeHist(img)\n",
    "    return TF.to_tensor(img)\n",
    "\n",
    "augmentations = [vertical_flip, horizontal_flip, brightness, color, sharpness, rotation, gaussian_blur, lambda img: random_crop(img, size=(64, 64)), add_noise, affine_transform, elastic_deformation, contrast, saturation, hue, perspective_transform, channel_shuffle, random_erasing, histogram_equalization]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code defines a function to augment an image multiple times using a Recurrent Independent Mechanisms (RIMs) model. The goal is to generate a set of augmented images while dynamically adjusting augmentation parameters based on model performance metrics.\n",
    "\n",
    "The function \"adjust_factor_based_on_performance\" adjusts the augmentation factor based on performance metrics. If the performance is above a certain threshold, the factor is increased; otherwise, it is decreased. This helps in fine-tuning the augmentations dynamically.\n",
    "\n",
    "The main function \"augment_image_rims\" takes an image, a list of augmentation functions, a RIMs model, and the number of augmentations to generate (B). It first converts the image to a tensor if it is not already one and initializes hidden and cell states for the RIMs model. Then, it iteratively applies augmentations B times. In each iteration, the RIMs model processes the image tensor to produce attention weights and hyperparameters, selects an augmentation based on the attention weights, and adjusts the factor for certain augmentations based on previous performance metrics. The selected augmentation is applied to the image, and the result is stored. Performance metrics are updated using dummy values for illustration.\n",
    "\n",
    "Finally, the function returns a list of augmented images and a summary DataFrame containing details about the augmentations applied. This approach leverages the RIMs model to intelligently select and adjust augmentations, aiming to improve the diversity and effectiveness of the augmented images based on dynamic performance feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1715970684858,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "9WXWL3Gxh3Cr"
   },
   "outputs": [],
   "source": [
    "def adjust_factor_based_on_performance(factor, performance, threshold=0.75):\n",
    "    if performance > threshold:\n",
    "        return factor * random.uniform(1.0, 1.5)\n",
    "    else:\n",
    "        return factor * random.uniform(0.5, 1.0)\n",
    "\n",
    "def augment_image_rims(img, augmentations, rims, B=15):\n",
    "    assert len(augmentations) > 0, \"No augmentations provided.\"\n",
    "    images = [img]\n",
    "    aug_names = []\n",
    "    aug_factors = []\n",
    "\n",
    "    device = rims.device\n",
    "    img_tensor = TF.to_tensor(img).unsqueeze(0).to(device) if not isinstance(img, torch.Tensor) else img.unsqueeze(0).to(device)\n",
    "\n",
    "    batch_size = img_tensor.size(0)\n",
    "    input_size = img_tensor.numel()\n",
    "    img_tensor = img_tensor.view(1, 1, input_size)\n",
    "\n",
    "    h = torch.zeros(rims.n_layers * rims.num_directions, batch_size, rims.hidden_size * rims.num_units).to(device)\n",
    "    c = torch.zeros(rims.n_layers * rims.num_directions, batch_size, rims.hidden_size * rims.num_units).to(device)\n",
    "\n",
    "    linear = nn.Linear(rims.hidden_size * rims.num_units, len(augmentations)).to(device)\n",
    "\n",
    "    for i in range(B):\n",
    "        outputs, h, c = rims(img_tensor, h, c)\n",
    "        preds = linear(outputs)\n",
    "        attention_weights = torch.softmax(preds, dim=-1)\n",
    "\n",
    "        # Introduce entropy regularization\n",
    "        entropy = -torch.sum(attention_weights * torch.log(attention_weights + 1e-10))\n",
    "        attention_weights = attention_weights + 0.1 * entropy\n",
    "\n",
    "        hyperparameters = torch.tanh(outputs).mean(dim=-1)\n",
    "\n",
    "        aug_idx = torch.multinomial(attention_weights.squeeze(), 1).item()\n",
    "        augmentation = augmentations[aug_idx]\n",
    "\n",
    "        if augmentation.__name__ in ['brightness', 'color', 'sharpness']:\n",
    "            factor = (hyperparameters[0].item() + 1) / 2\n",
    "            factor = adjust_factor_based_on_performance(factor, random.uniform(0, 1))  # Dummy performance value used here\n",
    "            diverse_factor = random.uniform(0.5, 1.5)\n",
    "            augmented_img = augmentation(img, factor_range=(factor * diverse_factor, factor * diverse_factor))\n",
    "            aug_factors.append(factor * diverse_factor)\n",
    "        else:\n",
    "            augmented_img = augmentation(img)\n",
    "            aug_factors.append(None)\n",
    "\n",
    "        images.append(augmented_img)\n",
    "        aug_names.append(augmentation.__name__)\n",
    "\n",
    "    df_aug_summary = pd.DataFrame({'Augmentation': aug_names, 'Factor': aug_factors})\n",
    "    return images, df_aug_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img, augmentations, B=15):\n",
    "    assert len(augmentations) > 0, \"There are no augmentations provided.\"\n",
    "\n",
    "    images = [img]\n",
    "    aug_names = []  # List to store augmentation names\n",
    "    aug_factors = []  # List to store augmentation factors\n",
    "\n",
    "    for i in range(B):\n",
    "        # Randomly choose an augmentation from the augmentation functions\n",
    "        index = random.randrange(0, len(augmentations))\n",
    "        augmentation = augmentations[index]\n",
    "        \n",
    "        if augmentation.__name__ in ['brightness', 'color', 'sharpness']:\n",
    "            factor = random.uniform(0.5, 1.5)  # Generate a random factor between 0.5 and 1.5\n",
    "            augmented_img = augmentation(img, factor_range=(factor, factor))\n",
    "            aug_factors.append(factor)\n",
    "        else:\n",
    "            augmented_img = augmentation(img)\n",
    "            aug_factors.append(None)\n",
    "        \n",
    "        # Add the augmented image to the list of images to evaluate\n",
    "        images.append(augmented_img)\n",
    "        aug_names.append(augmentation.__name__)  # Store the augmentation name\n",
    "\n",
    "        # Print the image every 10 iterations\n",
    "        #if i % 10 == 0 and i != 0:\n",
    "            #plt.imshow(augmented_img.permute(1, 2, 0))\n",
    "            #plt.title(f\"Iteration {i} - Augmentation: {augmentation.__name__}\")\n",
    "            #plt.show()\n",
    "\n",
    "    # Create a summary table\n",
    "    df_aug_summary = pd.DataFrame({'Augmentation': aug_names, 'Factor': aug_factors})\n",
    "    print(\"\\nAugmentation Summary Table:\")\n",
    "    print(df_aug_summary)\n",
    "\n",
    "    print(\"Number of images:\", len(images))\n",
    "\n",
    "    return images, df_aug_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1715970687565,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "xc5j-FiVdoBs"
   },
   "outputs": [],
   "source": [
    "# define the cost function used to evaluate the model output\n",
    "def get_cost_function():\n",
    "  cost_function = torch.nn.CrossEntropyLoss()\n",
    "  return cost_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1715970701773,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "pxiafvsgPisv"
   },
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "def get_optimizer(net, lr, wd, momentum):\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from imagenet (1000 labels) to imagenet-a (200 labels)\n",
    "# mapping obtained from: \n",
    "thousand_k_to_200 = {0: -1, 1: -1, 2: -1, 3: -1, 4: -1, 5: -1, 6: 0, 7: -1, 8: -1, 9: -1, 10: -1, 11: 1, 12: -1, 13: 2, 14: -1, 15: 3, 16: -1, 17: 4, 18: -1, 19: -1, 20: -1, 21: -1, 22: 5, 23: 6, 24: -1, 25: -1, 26: -1, 27: 7, 28: -1, 29: -1, 30: 8, 31: -1, 32: -1, 33: -1, 34: -1, 35: -1, 36: -1, 37: 9, 38: -1, 39: 10, 40: -1, 41: -1, 42: 11, 43: -1, 44: -1, 45: -1, 46: -1, 47: 12, 48: -1, 49: -1, 50: 13, 51: -1, 52: -1, 53: -1, 54: -1, 55: -1, 56: -1, 57: 14, 58: -1, 59: -1, 60: -1, 61: -1, 62: -1, 63: -1, 64: -1, 65: -1, 66: -1, 67: -1, 68: -1, 69: -1, 70: 15, 71: 16, 72: -1, 73: -1, 74: -1, 75: -1, 76: 17, 77: -1, 78: -1, 79: 18, 80: -1, 81: -1, 82: -1, 83: -1, 84: -1, 85: -1, 86: -1, 87: -1, 88: -1, 89: 19, 90: 20, 91: -1, 92: -1, 93: -1, 94: 21, 95: -1, 96: 22, 97: 23, 98: -1, 99: 24, 100: -1, 101: -1, 102: -1, 103: -1, 104: -1, 105: 25, 106: -1, 107: 26, 108: 27, 109: -1, 110: 28, 111: -1, 112: -1, 113: 29, 114: -1, 115: -1, 116: -1, 117: -1, 118: -1, 119: -1, 120: -1, 121: -1, 122: -1, 123: -1, 124: 30, 125: 31, 126: -1, 127: -1, 128: -1, 129: -1, 130: 32, 131: -1, 132: 33, 133: -1, 134: -1, 135: -1, 136: -1, 137: -1, 138: -1, 139: -1, 140: -1, 141: -1, 142: -1, 143: 34, 144: 35, 145: -1, 146: -1, 147: -1, 148: -1, 149: -1, 150: 36, 151: 37, 152: -1, 153: -1, 154: -1, 155: -1, 156: -1, 157: -1, 158: -1, 159: -1, 160: -1, 161: -1, 162: -1, 163: -1, 164: -1, 165: -1, 166: -1, 167: -1, 168: -1, 169: -1, 170: -1, 171: -1, 172: -1, 173: -1, 174: -1, 175: -1, 176: -1, 177: -1, 178: -1, 179: -1, 180: -1, 181: -1, 182: -1, 183: -1, 184: -1, 185: -1, 186: -1, 187: -1, 188: -1, 189: -1, 190: -1, 191: -1, 192: -1, 193: -1, 194: -1, 195: -1, 196: -1, 197: -1, 198: -1, 199: -1, 200: -1, 201: -1, 202: -1, 203: -1, 204: -1, 205: -1, 206: -1, 207: 38, 208: -1, 209: -1, 210: -1, 211: -1, 212: -1, 213: -1, 214: -1, 215: -1, 216: -1, 217: -1, 218: -1, 219: -1, 220: -1, 221: -1, 222: -1, 223: -1, 224: -1, 225: -1, 226: -1, 227: -1, 228: -1, 229: -1, 230: -1, 231: -1, 232: -1, 233: -1, 234: 39, 235: 40, 236: -1, 237: -1, 238: -1, 239: -1, 240: -1, 241: -1, 242: -1, 243: -1, 244: -1, 245: -1, 246: -1, 247: -1, 248: -1, 249: -1, 250: -1, 251: -1, 252: -1, 253: -1, 254: 41, 255: -1, 256: -1, 257: -1, 258: -1, 259: -1, 260: -1, 261: -1, 262: -1, 263: -1, 264: -1, 265: -1, 266: -1, 267: -1, 268: -1, 269: -1, 270: -1, 271: -1, 272: -1, 273: -1, 274: -1, 275: -1, 276: -1, 277: 42, 278: -1, 279: -1, 280: -1, 281: -1, 282: -1, 283: 43, 284: -1, 285: -1, 286: -1, 287: 44, 288: -1, 289: -1, 290: -1, 291: 45, 292: -1, 293: -1, 294: -1, 295: 46, 296: -1, 297: -1, 298: 47, 299: -1, 300: -1, 301: 48, 302: -1, 303: -1, 304: -1, 305: -1, 306: 49, 307: 50, 308: 51, 309: 52, 310: 53, 311: 54, 312: -1, 313: 55, 314: 56, 315: 57, 316: -1, 317: 58, 318: -1, 319: 59, 320: -1, 321: -1, 322: -1, 323: 60, 324: 61, 325: -1, 326: 62, 327: 63, 328: -1, 329: -1, 330: 64, 331: -1, 332: -1, 333: -1, 334: 65, 335: 66, 336: 67, 337: -1, 338: -1, 339: -1, 340: -1, 341: -1, 342: -1, 343: -1, 344: -1, 345: -1, 346: -1, 347: 68, 348: -1, 349: -1, 350: -1, 351: -1, 352: -1, 353: -1, 354: -1, 355: -1, 356: -1, 357: -1, 358: -1, 359: -1, 360: -1, 361: 69, 362: -1, 363: 70, 364: -1, 365: -1, 366: -1, 367: -1, 368: -1, 369: -1, 370: -1, 371: -1, 372: 71, 373: -1, 374: -1, 375: -1, 376: -1, 377: -1, 378: 72, 379: -1, 380: -1, 381: -1, 382: -1, 383: -1, 384: -1, 385: -1, 386: 73, 387: -1, 388: -1, 389: -1, 390: -1, 391: -1, 392: -1, 393: -1, 394: -1, 395: -1, 396: -1, 397: 74, 398: -1, 399: -1, 400: 75, 401: 76, 402: 77, 403: -1, 404: 78, 405: -1, 406: -1, 407: 79, 408: -1, 409: -1, 410: -1, 411: 80, 412: -1, 413: -1, 414: -1, 415: -1, 416: 81, 417: 82, 418: -1, 419: -1, 420: 83, 421: -1, 422: -1, 423: -1, 424: -1, 425: 84, 426: -1, 427: -1, 428: 85, 429: -1, 430: 86, 431: -1, 432: -1, 433: -1, 434: -1, 435: -1, 436: -1, 437: 87, 438: 88, 439: -1, 440: -1, 441: -1, 442: -1, 443: -1, 444: -1, 445: 89, 446: -1, 447: -1, 448: -1, 449: -1, 450: -1, 451: -1, 452: -1, 453: -1, 454: -1, 455: -1, 456: 90, 457: 91, 458: -1, 459: -1, 460: -1, 461: 92, 462: 93, 463: -1, 464: -1, 465: -1, 466: -1, 467: -1, 468: -1, 469: -1, 470: 94, 471: -1, 472: 95, 473: -1, 474: -1, 475: -1, 476: -1, 477: -1, 478: -1, 479: -1, 480: -1, 481: -1, 482: -1, 483: 96, 484: -1, 485: -1, 486: 97, 487: -1, 488: 98, 489: -1, 490: -1, 491: -1, 492: 99, 493: -1, 494: -1, 495: -1, 496: 100, 497: -1, 498: -1, 499: -1, 500: -1, 501: -1, 502: -1, 503: -1, 504: -1, 505: -1, 506: -1, 507: -1, 508: -1, 509: -1, 510: -1, 511: -1, 512: -1, 513: -1, 514: 101, 515: -1, 516: 102, 517: -1, 518: -1, 519: -1, 520: -1, 521: -1, 522: -1, 523: -1, 524: -1, 525: -1, 526: -1, 527: -1, 528: 103, 529: -1, 530: 104, 531: -1, 532: -1, 533: -1, 534: -1, 535: -1, 536: -1, 537: -1, 538: -1, 539: 105, 540: -1, 541: -1, 542: 106, 543: 107, 544: -1, 545: -1, 546: -1, 547: -1, 548: -1, 549: 108, 550: -1, 551: -1, 552: 109, 553: -1, 554: -1, 555: -1, 556: -1, 557: 110, 558: -1, 559: -1, 560: -1, 561: 111, 562: 112, 563: -1, 564: -1, 565: -1, 566: -1, 567: -1, 568: -1, 569: 113, 570: -1, 571: -1, 572: 114, 573: 115, 574: -1, 575: 116, 576: -1, 577: -1, 578: -1, 579: 117, 580: -1, 581: -1, 582: -1, 583: -1, 584: -1, 585: -1, 586: -1, 587: -1, 588: -1, 589: 118, 590: -1, 591: -1, 592: -1, 593: -1, 594: -1, 595: -1, 596: -1, 597: -1, 598: -1, 599: -1, 600: -1, 601: -1, 602: -1, 603: -1, 604: -1, 605: -1, 606: 119, 607: 120, 608: -1, 609: 121, 610: -1, 611: -1, 612: -1, 613: -1, 614: 122, 615: -1, 616: -1, 617: -1, 618: -1, 619: -1, 620: -1, 621: -1, 622: -1, 623: -1, 624: -1, 625: -1, 626: 123, 627: 124, 628: -1, 629: -1, 630: -1, 631: -1, 632: -1, 633: -1, 634: -1, 635: -1, 636: -1, 637: -1, 638: -1, 639: -1, 640: 125, 641: 126, 642: 127, 643: 128, 644: -1, 645: -1, 646: -1, 647: -1, 648: -1, 649: -1, 650: -1, 651: -1, 652: -1, 653: -1, 654: -1, 655: -1, 656: -1, 657: -1, 658: 129, 659: -1, 660: -1, 661: -1, 662: -1, 663: -1, 664: -1, 665: -1, 666: -1, 667: -1, 668: 130, 669: -1, 670: -1, 671: -1, 672: -1, 673: -1, 674: -1, 675: -1, 676: -1, 677: 131, 678: -1, 679: -1, 680: -1, 681: -1, 682: 132, 683: -1, 684: 133, 685: -1, 686: -1, 687: 134, 688: -1, 689: -1, 690: -1, 691: -1, 692: -1, 693: -1, 694: -1, 695: -1, 696: -1, 697: -1, 698: -1, 699: -1, 700: -1, 701: 135, 702: -1, 703: -1, 704: 136, 705: -1, 706: -1, 707: -1, 708: -1, 709: -1, 710: -1, 711: -1, 712: -1, 713: -1, 714: -1, 715: -1, 716: -1, 717: -1, 718: -1, 719: 137, 720: -1, 721: -1, 722: -1, 723: -1, 724: -1, 725: -1, 726: -1, 727: -1, 728: -1, 729: -1, 730: -1, 731: -1, 732: -1, 733: -1, 734: -1, 735: -1, 736: 138, 737: -1, 738: -1, 739: -1, 740: -1, 741: -1, 742: -1, 743: -1, 744: -1, 745: -1, 746: 139, 747: -1, 748: -1, 749: 140, 750: -1, 751: -1, 752: 141, 753: -1, 754: -1, 755: -1, 756: -1, 757: -1, 758: 142, 759: -1, 760: -1, 761: -1, 762: -1, 763: 143, 764: -1, 765: 144, 766: -1, 767: -1, 768: 145, 769: -1, 770: -1, 771: -1, 772: -1, 773: 146, 774: 147, 775: -1, 776: 148, 777: -1, 778: -1, 779: 149, 780: 150, 781: -1, 782: -1, 783: -1, 784: -1, 785: -1, 786: 151, 787: -1, 788: -1, 789: -1, 790: -1, 791: -1, 792: 152, 793: -1, 794: -1, 795: -1, 796: -1, 797: 153, 798: -1, 799: -1, 800: -1, 801: -1, 802: 154, 803: 155, 804: 156, 805: -1, 806: -1, 807: -1, 808: -1, 809: -1, 810: -1, 811: -1, 812: -1, 813: 157, 814: -1, 815: 158, 816: -1, 817: -1, 818: -1, 819: -1, 820: 159, 821: -1, 822: -1, 823: 160, 824: -1, 825: -1, 826: -1, 827: -1, 828: -1, 829: -1, 830: -1, 831: 161, 832: -1, 833: 162, 834: -1, 835: 163, 836: -1, 837: -1, 838: -1, 839: 164, 840: -1, 841: -1, 842: -1, 843: -1, 844: -1, 845: 165, 846: -1, 847: 166, 848: -1, 849: -1, 850: 167, 851: -1, 852: -1, 853: -1, 854: -1, 855: -1, 856: -1, 857: -1, 858: -1, 859: 168, 860: -1, 861: -1, 862: 169, 863: -1, 864: -1, 865: -1, 866: -1, 867: -1, 868: -1, 869: -1, 870: 170, 871: -1, 872: -1, 873: -1, 874: -1, 875: -1, 876: -1, 877: -1, 878: -1, 879: 171, 880: 172, 881: -1, 882: -1, 883: -1, 884: -1, 885: -1, 886: -1, 887: -1, 888: 173, 889: -1, 890: 174, 891: -1, 892: -1, 893: -1, 894: -1, 895: -1, 896: -1, 897: 175, 898: -1, 899: -1, 900: 176, 901: -1, 902: -1, 903: -1, 904: -1, 905: -1, 906: -1, 907: 177, 908: -1, 909: -1, 910: -1, 911: -1, 912: -1, 913: 178, 914: -1, 915: -1, 916: -1, 917: -1, 918: -1, 919: -1, 920: -1, 921: -1, 922: -1, 923: -1, 924: 179, 925: -1, 926: -1, 927: -1, 928: -1, 929: -1, 930: -1, 931: -1, 932: 180, 933: 181, 934: 182, 935: -1, 936: -1, 937: 183, 938: -1, 939: -1, 940: -1, 941: -1, 942: -1, 943: 184, 944: -1, 945: 185, 946: -1, 947: 186, 948: -1, 949: -1, 950: -1, 951: 187, 952: -1, 953: -1, 954: 188, 955: -1, 956: 189, 957: 190, 958: -1, 959: 191, 960: -1, 961: -1, 962: -1, 963: -1, 964: -1, 965: -1, 966: -1, 967: -1, 968: -1, 969: -1, 970: -1, 971: 192, 972: 193, 973: -1, 974: -1, 975: -1, 976: -1, 977: -1, 978: -1, 979: -1, 980: 194, 981: 195, 982: -1, 983: -1, 984: 196, 985: -1, 986: 197, 987: 198, 988: 199, 989: -1, 990: -1, 991: -1, 992: -1, 993: -1, 994: -1, 995: -1, 996: -1, 997: -1, 998: -1, 999: -1}\n",
    "indices_in_1k = [k for k in thousand_k_to_200 if thousand_k_to_200[k] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the output distribution\n",
    "def get_predictions(images, model, transforms, device):\n",
    "  # collect the prediction for every image in input\n",
    "  img_results = []\n",
    "  for img in images:\n",
    "    single_batch = transforms(img).unsqueeze(0).to(device)\n",
    "    prediction = model(single_batch).squeeze(0)\n",
    "    img_results.append(prediction[indices_in_1k])\n",
    "  return torch.stack(img_results).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the marginal cross entropy\n",
    "def marginal_cross_entropy(marginal_dist, labels, cost_function):\n",
    "  entropy = 0.0\n",
    "  # sum all entropies for the different labels since I don't know the real one\n",
    "  for label in labels:\n",
    "    entropy += cost_function(marginal_dist, label)\n",
    "  return entropy\n",
    "\n",
    "def marginal_entropy_loss(logits):\n",
    "    \"\"\"\n",
    "    Computes the entropy of the marginal output distribution over multiple augmentations.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): A tensor of shape (N, C), where N is the number of augmentations\n",
    "                               and C is the number of classes. Each row corresponds to the logits\n",
    "                               produced by the model for one augmentation.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The scalar value of the marginal entropy loss.\n",
    "    \"\"\"\n",
    "    # Step 1: Normalize logits to get log probabilities\n",
    "    log_probs = logits - logits.logsumexp(dim=-1, keepdim=True)\n",
    "\n",
    "    # Step 2: Compute the average log probabilities over the augmentations\n",
    "    avg_log_probs = log_probs.logsumexp(dim=0) - torch.log(torch.tensor(logits.shape[0], dtype=log_probs.dtype))\n",
    "\n",
    "    # Step 3: Calculate the marginal probability distribution (exp of avg_log_probs)\n",
    "    avg_probs = torch.exp(avg_log_probs)\n",
    "\n",
    "    # Step 4: Compute the entropy of the marginal distribution\n",
    "    entropy_loss = -(avg_probs * avg_log_probs).sum()\n",
    "\n",
    "    return entropy_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test time robustness via MEMO algorithm\n",
    "def adapt(model, test_sample, B, optimizer, transforms, device, rims):\n",
    "  \n",
    "  with torch.enable_grad():\n",
    "    # get the B + 1 images\n",
    "    # augmented_images = augment_image(test_sample, augmentations, B)\n",
    "    augmented_images, summary_table = augment_image_rims(test_sample, augmentations, rims, B)\n",
    "\n",
    "    # get results for each image\n",
    "    raw_logits = get_predictions(augmented_images, model, transforms, device)\n",
    "\n",
    "    # compute marginal distribution on raw logits\n",
    "    loss = marginal_entropy_loss(raw_logits)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1715970703645,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "bhN-AaM_huqQ"
   },
   "outputs": [],
   "source": [
    "# not using this function right now because compute the marginal distribution\n",
    "# in marginal_entropy_loss function\n",
    "\n",
    "# compute the marginal output distribution\n",
    "def marginal_distribution(images, model, transforms, device):\n",
    "  # collect the prediction for every image in input\n",
    "  img_results = []\n",
    "  for img in images:\n",
    "    single_batch = transforms(img).unsqueeze(0).to(device)\n",
    "    prediction = model(single_batch).squeeze(0).softmax(0)\n",
    "    img_results.append(prediction)\n",
    "\n",
    "  # sum all the resulting tensors\n",
    "  sum_results = torch.sum(torch.stack(img_results), dim=0).to(device)\n",
    "  # divide each element by B to obtain the marginal output distribution\n",
    "  num_images = len(images)\n",
    "  res = torch.div(sum_results, num_images).to(device)\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1715971147154,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "lq-B2QHFufkB"
   },
   "outputs": [],
   "source": [
    "# not using this function right now because for now we compute the loss\n",
    "# without the use of labels by using the marginal entropy\n",
    "\n",
    "# compute the marginal cross entropy\n",
    "def marginal_cross_entropy(marginal_dist, labels, cost_function):\n",
    "  entropy = 0.0\n",
    "  # sum all entropies for the different labels since I don't know the real one\n",
    "  for label in labels:\n",
    "    entropy += cost_function(marginal_dist, label)\n",
    "  return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1715971150673,
     "user": {
      "displayName": "Edoardo Cecchinato",
      "userId": "16439300506635722937"
     },
     "user_tz": -120
    },
    "id": "_cmtwAFRFCoW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ttr_MEMO_with_rims(model, data_loader, B, optimizer, transforms, rims, device=\"cuda\"):\n",
    "    correct = []\n",
    "\n",
    "    # save the original model weights\n",
    "    original_params = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # set the network to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # iterate over the test set\n",
    "    for batch_idx, (inputs, targets) in tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Testing\", leave=False):\n",
    "        # Load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # apply MEMO to each test point in the batch\n",
    "        intermediate_outputs = []\n",
    "        for input, label in zip(inputs, targets):\n",
    "            # adapt weights by performing marginal output minimization\n",
    "            adapt(model, input, B, optimizer, transforms, device, rims)\n",
    "            # intermediate_outputs.append(output)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input.unsqueeze(0))[:, indices_in_1k]\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                confidence = torch.nn.functional.softmax(outputs, dim=1).squeeze()[predicted].item() \n",
    "            correctness = 1 if predicted.item() == label.item() else 0\n",
    "            correct.append(correctness)\n",
    "\n",
    "            # reapply original weights to the model\n",
    "            model.load_state_dict(original_params)\n",
    "\n",
    "        # if (batch_idx+1) % 10 == 0:\n",
    "        #     print(f'MEMO adapt test error {(1-np.mean(correct))*100:.2f}')\n",
    "    print(f'Final MEMO adapt test error {(1-np.mean(correct))*100:.2f}')\n",
    "\n",
    "\n",
    "def main_with_rims(\n",
    "    run_name,\n",
    "    batch_size=32,  # Adjusted batch size\n",
    "    device=\"cuda\",\n",
    "    learning_rate=0.0001,  # Reduced learning rate\n",
    "    weight_decay=0.00001,  # Adjusted weight decay\n",
    "    momentum=0.9,\n",
    "    num_augmentations=15\n",
    "):\n",
    "    device = torch.device(device)\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    model = resnet50(weights=weights).to(device)\n",
    "    preprocess = weights.transforms()\n",
    "\n",
    "    # Initialize the test dataloader\n",
    "    test_loader, _ = get_data(batch_size, data_folder, preprocess)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = get_optimizer(model, learning_rate, weight_decay, momentum)\n",
    "\n",
    "    # Initialize the cost function\n",
    "    # cost_function = get_cost_function()\n",
    "\n",
    "    # Get a sample image to determine the input dimension\n",
    "    sample_img, _ = next(iter(test_loader))\n",
    "    sample_img = sample_img[0].unsqueeze(0)  # Take one sample and add batch dimension\n",
    "    sample_img = preprocess(sample_img)\n",
    "    input_dim = sample_img.size(1) * sample_img.size(2) * sample_img.size(3)  # Channels * Height * Width\n",
    "\n",
    "    hidden_dim = 128\n",
    "    num_units = 4\n",
    "    k = 2\n",
    "\n",
    "    rims = RIM('cuda', input_dim, hidden_dim, num_units, k, rnn_cell='LSTM', n_layers=2, bidirectional=False).to(device)\n",
    "    print(f\"Initialized RIMs with input_dim={input_dim}, hidden_dim={hidden_dim}, num_units={num_units}, k={k}\")\n",
    "\n",
    "    # Run the test\n",
    "    ttr_MEMO_with_rims(model, test_loader, num_augmentations, optimizer, preprocess, rims, device)\n",
    "    # test_loss, test_accuracy, combined_summary_table = test(model, test_loader, num_augmentations, optimizer, preprocess, rims, device)\n",
    "    # print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "\n",
    "    # Print the combined summary table\n",
    "    # print(\"\\nCombined Augmentation Summary Table:\")\n",
    "    # print(combined_summary_table)\n",
    "\n",
    "    # Calculate statistics\n",
    "    # aug_counts = combined_summary_table['Augmentation'].value_counts().reset_index()\n",
    "    # aug_counts.columns = ['Augmentation', 'Count']\n",
    "\n",
    "    # aug_factors = combined_summary_table.groupby('Augmentation')['Factor'].mean().reset_index()\n",
    "    # aug_factors.columns = ['Augmentation', 'Average Factor']\n",
    "\n",
    "    # final_summary = pd.merge(aug_counts, aug_factors, on='Augmentation', how='left')\n",
    "\n",
    "    # print(\"\\nFinal Summary Table:\")\n",
    "    # print(final_summary)\n",
    "\n",
    "main_with_rims(\"resnet_MEMO\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
