{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# DL Project 2023/24"],"metadata":{"id":"PDYnDRjPfNF_"}},{"cell_type":"markdown","source":["## Introduction\n","\n","Description of the method choosen and the work done"],"metadata":{"id":"ll5uAwzhfYHn"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Lw7k2mxRfE1D","executionInfo":{"status":"ok","timestamp":1715970600487,"user_tz":-120,"elapsed":19732,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"outputs":[],"source":["# import modules\n","import torch\n","import torchvision\n","from torchvision.models import resnet50, ResNet50_Weights\n","import torchvision.transforms as transforms\n","import torchvision.transforms.functional as TF\n","from torch.utils.tensorboard import SummaryWriter\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQnIh3lo3VAS","executionInfo":{"status":"ok","timestamp":1715970642796,"user_tz":-120,"elapsed":37897,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}},"outputId":"94b3fb3e-e93d-4bf2-eceb-79b231353554"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Reading Data"],"metadata":{"id":"MqX-jwMHgZ3G"}},{"cell_type":"code","source":["import tarfile\n","import os\n","import shutil\n","import re\n","\n","tar_file = \"./drive/MyDrive/DL_project/imagenet-a.tar\"\n","data_folder = \"imagenet-a\"\n","\n","# function to untar the dataset and store it in a new folder\n","def extract_dataset(compress_file, destination_folder):\n","  # function to change dir names to their words description\n","  def change_folders_names(readme_file, dataset_root):\n","    with open(readme_file, 'r') as f:\n","        lines = f.readlines()\n","        for line in lines:\n","            # Match lines containing WordNet IDs and descriptions\n","            match = re.match(r'n\\d+ (.+)', line)\n","            if match:\n","                # Split the line into WordNet ID and description\n","                parts = match.group(0).split()\n","                wordnet_id = parts[0]\n","                description = ' '.join(parts[1:])\n","                os.rename(os.path.join(dataset_root, wordnet_id),\n","                            os.path.join(dataset_root, description))\n","\n","  if not os.path.exists(compress_file):\n","    print(\"Compress file doesn't exist.\")\n","    return\n","\n","  if os.path.exists(destination_folder):\n","    # remove the folder if already exists one\n","    shutil.rmtree(destination_folder)\n","\n","  # extract content from the .tar file\n","  with tarfile.open(compress_file, 'r') as tar_ref:\n","    tar_ref.extractall(\"./\")\n","  print(\"All the data is extracted.\")\n","\n","  change_folders_names(destination_folder+\"/README.txt\", destination_folder)\n","\n","extract_dataset(tar_file, data_folder)"],"metadata":{"id":"4h7Y7SSrgc7y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715970667457,"user_tz":-120,"elapsed":21703,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}},"outputId":"bd874e65-9065-46ec-947e-8a4ae09e067e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["All the data is extracted.\n"]}]},{"cell_type":"code","source":["ids_list = os.listdir(data_folder)\n","print(ids_list)\n","len(ids_list) # 200 folders + 1 readme"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rzBAWU06L_Mm","executionInfo":{"status":"ok","timestamp":1715970670636,"user_tz":-120,"elapsed":592,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}},"outputId":"2a005eca-33f2-45bb-ea03-a7427ee7fa99"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['goldfinch', 'README.txt', 'parking meter', 'balloon', 'cottontail rabbit', 'pufferfish', 'salt shaker', 'green iguana', 'fountain', 'sulphur-crested cockatoo', 'weevil', 'mosque', 'monarch butterfly', 'saxophone', 'goblet', 'German Shepherd Dog', 'cello', 'go-kart', 'baseball player', 'ocarina', 'bell pepper', 'Rottweiler', 'ant', 'unicycle', 'Chihuahua', 'billiard table', 'stingray', 'manhole cover', 'junco', 'flagpole', 'syringe', 'wine bottle', 'hair dryer', 'jellyfish', 'academic gown', 'Persian cat', 'fly', 'barn', 'hummingbird', 'rapeseed', 'accordion', 'snowmobile', 'white-headed capuchin', 'schooner', 'cheeseburger', 'box turtle', 'dumbbell', 'broom', 'centipede', 'dragonfly', 'banjo', 'racket', 'agama', 'reel', 'basketball', 'apron', 'goose', 'breastplate', 'chain', 'sleeping bag', 'volleyball', 'jay', 'stick insect', 'cockroach', 'leafhopper', 'mongoose', 'golf cart', 'nail', 'suspension bridge', 'acoustic guitar', 'maraca', 'pretzel', 'tricycle', 'sea lion', 'great egret', 'sea anemone', 'quill', 'spatula', 'torch', 'rhinoceros beetle', 'forklift', 'rugby ball', 'newt', 'bald eagle', 'harvestman', 'obelisk', 'submarine', 'custard apple', 'carbonara', 'Golden Retriever', 'teddy bear', 'parachute', 'soap dispenser', 'lorikeet', 'marmot', 'guacamole', 'African bush elephant', 'umbrella', 'bow', 'banana', 'hockey puck', 'airliner', 'snail', 'wheelbarrow', 'American bullfrog', 'couch', 'rotary dial telephone', 'sundial', 'mitten', 'lion', 'cowboy boot', 'sandal', 'garbage truck', 'grand piano', 'viaduct', 'American black bear', 'skunk', 'bikini', 'corn', 'ladybug', 'lynx', 'jeep', 'steam locomotive', 'pomegranate', 'flamingo', 'hermit crab', 'Christmas stocking', 'water tower', 'pelican', 'toucan', 'cucumber', 'oystercatcher', 'digital clock', 'kimono', \"yellow lady's slipper\", 'marimba', 'small white', 'bubble', 'baboon', 'revolver', 'shovel', 'stethoscope', 'lemon', 'starfish', 'mushroom', 'crayfish', 'spider web', 'chameleon', 'sewing machine', 'gossamer-winged butterfly', 'bee', 'limousine', 'garter snake', 'bison', 'bow tie', 'grasshopper', 'doormat', 'armadillo', \"jack-o'-lantern\", 'clothes iron', 'vulture', 'drumstick', 'canoe', 'feather boa', 'beaker', 'rocking chair', 'cradle', 'toaster', 'hot dog', 'duck', 'ambulance', 'pug', 'lighthouse', 'shipwreck', 'snowplow', 'flatworm', 'organ', 'volcano', 'broccoli', 'red fox', 'American alligator', 'chest', 'piggy bank', 'scorpion', 'American robin', 'lighter', 'koala', 'tarantula', 'cliff', 'balance beam', 'castle', 'acorn', 'school bus', 'mantis', 'fox squirrel', 'washing machine', 'candle', 'envelope', 'mask', 'porcupine', 'tank']\n"]},{"output_type":"execute_result","data":{"text/plain":["201"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from PIL import Image\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","\n","# function that returns a DataLoader for the dataset\n","def get_data(batch_size, dataset_path, transform):\n","\n","  data = torchvision.datasets.ImageFolder(root=dataset_path, transform=transform)\n","\n","  class_labels = data.classes\n","  print(f\"The dataset contains {len(data)} images.\")\n","  print(f\"The dataset contains {len(class_labels)} labels.\")\n","\n","  test_loader = torch.utils.data.DataLoader(data, batch_size, shuffle=False, num_workers=8)\n","\n","  return test_loader, class_labels"],"metadata":{"id":"eul04CxP3VaH","executionInfo":{"status":"ok","timestamp":1715970673171,"user_tz":-120,"elapsed":590,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torchvision.transforms.functional as TF\n","\n","# function to display images from the DataLoader\n","def show_images(dataloader, num_images=5):\n","  # get a batch of data\n","  data_iter = iter(dataloader)\n","  images, labels = next(data_iter)\n","\n","  # convert images to numpy array\n","  images = images.numpy()\n","\n","  # display images\n","  fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n","  for i in range(num_images):\n","      image = np.transpose(images[i], (1, 2, 0))  # move channels in last position\n","      image = np.clip(image, 0, 1)\n","      axes[i].imshow(image)\n","      axes[i].axis('off')\n","      axes[i].set_title(dataloader.dataset.classes[labels[i]])\n","  plt.show()"],"metadata":{"id":"HHAElKCP7mMv","executionInfo":{"status":"ok","timestamp":1715970676875,"user_tz":-120,"elapsed":470,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## MEMO"],"metadata":{"id":"VxlU46PFgdSd"}},{"cell_type":"code","source":["from PIL import ImageOps, ImageEnhance\n","\n","# define some image augmentations\n","\n","def vertical_flip(img):\n","    img = TF.to_pil_image(img)\n","    res = img.transpose(Image.FLIP_TOP_BOTTOM)\n","    return TF.to_tensor(res)\n","\n","def brightness(img, factor_range=(0.5, 1.5)):\n","  img = TF.to_pil_image(img)\n","  factor = np.random.uniform(factor_range[0], factor_range[1])\n","  enhancer = ImageEnhance.Brightness(img)\n","  res = enhancer.enhance(factor)\n","  return TF.to_tensor(res)\n","\n","'''\n","def rotation(img, angle_range=(-45, 45)):\n","  angle = np.random.uniform(angle_range[0], angle_range[1])\n","  return img.rotate(angle)\n","'''\n","\n","def color(img, factor_range=(0.5, 1.5)):\n","  img = TF.to_pil_image(img)\n","  factor = np.random.uniform(factor_range[0], factor_range[1])\n","  enhancer = ImageEnhance.Color(img)\n","  res = enhancer.enhance(factor)\n","  return TF.to_tensor(res)\n","\n","def sharpness(img, factor_range=(0.5, 1.5)):\n","  img = TF.to_pil_image(img)\n","  factor = np.random.uniform(factor_range[0], factor_range[1])\n","  enhancer = ImageEnhance.Sharpness(img)\n","  res = enhancer.enhance(factor)\n","  return TF.to_tensor(res)\n","\n","augmentations = [vertical_flip, brightness, color, sharpness]"],"metadata":{"id":"tNxMhrlyj5IW","executionInfo":{"status":"ok","timestamp":1715970681225,"user_tz":-120,"elapsed":518,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","# functon that apply B augmentations to the original image and return M+1 images\n","def augment_image(img, augmentations, B=15):\n","  assert len(augmentations) > 0, \"There are not augmentations provided.\"\n","\n","  images = [img]\n","  for _ in range(B):\n","    # randomly choose an augmentation in the augmentation functions\n","    index = random.randrange(0, len(augmentations))\n","    augmentation = augmentations[index]\n","    # apply the augmentation to the original image\n","    augmented_img = augmentation(img)\n","    # add the augmented image to the list of images I want to evaluate\n","    images.append(augmented_img)\n","  return images"],"metadata":{"id":"9WXWL3Gxh3Cr","executionInfo":{"status":"ok","timestamp":1715970684858,"user_tz":-120,"elapsed":5,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# define the cost function used to evaluate the model output\n","def get_cost_function():\n","  cost_function = torch.nn.CrossEntropyLoss()\n","  return cost_function"],"metadata":{"id":"xc5j-FiVdoBs","executionInfo":{"status":"ok","timestamp":1715970687565,"user_tz":-120,"elapsed":470,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# define the optimizer\n","def get_optimizer(net, lr, wd, momentum):\n","    optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n","    return optimizer"],"metadata":{"id":"pxiafvsgPisv","executionInfo":{"status":"ok","timestamp":1715970701773,"user_tz":-120,"elapsed":556,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# compute the marginal output distribution\n","def marginal_distribution(images, model, transforms, device):\n","  # collect the prediction for every image in input\n","  img_results = []\n","  for img in images:\n","    single_batch = transforms(img).unsqueeze(0).to(device)\n","    prediction = model(single_batch).squeeze(0).softmax(0)\n","    img_results.append(prediction)\n","\n","  # sum all the resulting tensors\n","  sum_results = torch.sum(torch.stack(img_results), dim=0).to(device)\n","  # divide each element by B to obtain the marginal output distribution\n","  num_images = len(images)\n","  res = torch.div(sum_results, num_images).to(device)\n","  return res"],"metadata":{"id":"bhN-AaM_huqQ","executionInfo":{"status":"ok","timestamp":1715970703645,"user_tz":-120,"elapsed":6,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# compute the marginal cross entropy\n","def marginal_cross_entropy(marginal_dist, labels, cost_function):\n","  entropy = 0.0\n","  # sum all entropies for the different labels since I don't know the real one\n","  for label in labels:\n","    entropy += cost_function(marginal_dist, label)\n","  return entropy"],"metadata":{"id":"lq-B2QHFufkB","executionInfo":{"status":"ok","timestamp":1715971147154,"user_tz":-120,"elapsed":577,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import copy\n","\n","# test time robustness via MEMO algorithm\n","def ttr_MEMO(model, test_sample, labels, B, cost_function, optimizer, transforms, device):\n","  # save the original model weights\n","  original_params = copy.deepcopy(model.state_dict())\n","\n","  with torch.enable_grad():\n","    # get the B + 1 images\n","    augmented_images = augment_image(test_sample, augmentations, B)\n","\n","    # get the marginal output distribution\n","    marginal_dist = marginal_distribution(augmented_images, model, transforms, device)\n","\n","    # update the model weights\n","    loss = marginal_cross_entropy(marginal_dist, labels, cost_function)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","  test_sample = transforms(test_sample).unsqueeze(0).to(device)\n","  output = model(test_sample).squeeze(0).softmax(0)\n","\n","  # reapply original weights to the model\n","  model.load_state_dict(original_params)\n","\n","  return output"],"metadata":{"id":"_cmtwAFRFCoW","executionInfo":{"status":"ok","timestamp":1715971150673,"user_tz":-120,"elapsed":527,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## Test Procedure"],"metadata":{"id":"98wdJeghGZsv"}},{"cell_type":"code","source":["def test(model, data_loader, B, cost_function, optimizer, transforms, device=\"cuda\"):\n","  samples = 0.0\n","  cumulative_loss = 0.0\n","  cumulative_accuracy = 0.0\n","\n","  # set the network to evaluation mode\n","  model.eval()\n","\n","  # disable gradient computation for testing mode\n","  with torch.no_grad():\n","    # iterate over the test set\n","    for batch_idx, (inputs, targets) in enumerate(data_loader):\n","      # Load data into GPU\n","      inputs = inputs.to(device)\n","      targets = targets.to(device)\n","\n","      # forward pass\n","      batch_size = inputs.size(0)\n","      num_labels = 1000 # 1000 is the ImageNet number of labels\n","      empty_tensor = torch.empty(num_labels).to(device)\n","\n","      # apply MEMO to each test point in the batch\n","      intermediate_outputs = []\n","      for input in inputs:\n","        output = ttr_MEMO(model, input, targets, B, cost_function, optimizer, transforms, device)\n","        intermediate_outputs.append(output)\n","\n","      outputs = torch.stack(intermediate_outputs).to(device)\n","\n","      # outputs_no_MEMO = model(inputs)\n","\n","      # loss computation\n","      loss = cost_function(outputs, targets)\n","\n","      # fetch prediction and loss value\n","      samples+=inputs.shape[0]\n","      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n","      _, predicted = outputs.max(1)\n","\n","      # compute accuracy\n","      cumulative_accuracy += predicted.eq(targets).sum().item()\n","\n","  return cumulative_loss / samples, cumulative_accuracy / samples * 100"],"metadata":{"id":"BXSBVg2bGdlZ","executionInfo":{"status":"ok","timestamp":1715970785640,"user_tz":-120,"elapsed":4,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## Put all together"],"metadata":{"id":"-mjNvK8WYlEk"}},{"cell_type":"code","source":["def main(\n","    run_name,\n","    batch_size = 32,\n","    device = \"cuda\",\n","    learning_rate=0.001,\n","    weight_decay=0.000001,\n","    momentum=0.9,\n","    num_augmentations = 15\n","):\n","  # writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n","  device = device\n","\n","  # itialize the ResNet model\n","  weights = ResNet50_Weights.DEFAULT\n","  model = resnet50(weights=weights).to(device)\n","\n","  # initialize the inference transforms\n","  preprocess = weights.transforms()\n","  preprocess\n","\n","  # initialize the test dataloader\n","  test_loader, _ = get_data(batch_size, data_folder, preprocess)\n","\n","  # initialize the optimizer\n","  optimizer = get_optimizer(model, learning_rate, weight_decay, momentum)\n","\n","  # initialize the cost function\n","  cost_function = get_cost_function()\n","\n","  test_loss, test_accuracy = test(model, test_loader, num_augmentations, cost_function, optimizer, preprocess, device)\n","  print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")"],"metadata":{"id":"GWwZuurhYpza","executionInfo":{"status":"ok","timestamp":1715971213798,"user_tz":-120,"elapsed":571,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["main(\"resnet_MEMO\")"],"metadata":{"id":"lCWTd6ZxHqiE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7fd8d829-23d9-4bee-d038-0205844497b2","executionInfo":{"status":"ok","timestamp":1715974761520,"user_tz":-120,"elapsed":3543859,"user":{"displayName":"Edoardo Cecchinato","userId":"16439300506635722937"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["The dataset contains 7500 images.\n","The dataset contains 200 labels.\n","\tTest loss 0.21649, Test accuracy 0.04\n"]}]}]}